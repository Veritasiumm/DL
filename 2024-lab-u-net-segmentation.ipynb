{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcbb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "import gzip\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368198f",
   "metadata": {},
   "source": [
    "**Image segmentation** is an important task in  computer vision. The goal is to find multiple areas in an image and to assign labels to these area.  It provides a different kind of information than: \n",
    "- **image classification**  which caracterizes images with global labels;\n",
    "- **object detection** which usually relies on finding bounding-boxes around  detected objects\n",
    "\n",
    "Segmentation is useful and can be used in real-world applications such as medical imaging, clothes segmentation, flooding maps, self-driving cars, etc. There are two types of image segmentation:\n",
    "- Semantic segmentation: classify each pixel with a label.\n",
    "- Instance segmentation: classify each pixel and differentiate each object instance.\n",
    "\n",
    "U-Net is a semantic segmentation technique [originally proposed for medical imaging segmentation](https://arxiv.org/abs/1505.04597). Itâ€™s one of the earlier deep learning segmentation models. This architecture is still widely used in more advanced models like Generative Adversarial or Diffusion Network. \n",
    "\n",
    "The model architecture is fairly simple: an encoder (for downsampling) and a decoder (for upsampling) with skip connections. U-Net is only based on convolutions. More specifically, the output classification is done at the pixel level with a *(1,1)* convolution. It has therefore  the following advantages: \n",
    "- parameter and data efficiency, \n",
    "- independent of the input size. \n",
    "\n",
    "The following image is taken from the original paper:\n",
    "\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\"  width=\"60%\" height=\"30%\">\n",
    "\n",
    "\n",
    "The goal of this lab session is to develop the U-Net architecture for image semantic segmentation and we will consider a binary segmentation task.  \n",
    "\n",
    "#  Cell nuclei segmentation: the dataset\n",
    "\n",
    "Cell nuclei segmentation is an essential step in the biological analysis of microscopy images. \n",
    "This segmentation can be manually achieved with dedicated software, however it is very costly. \n",
    "In this lab session, the starting point is this [nature paper](https://www.nature.com/articles/s41597-020-00608-w). To quote some part of the paper: \n",
    "\n",
    "Fully-automated nuclear image segmentation is the prerequisite to ensure statistically significant, quantitative analyses of tissue preparations,applied in digital pathology or quantitative microscopy. The design of segmentation methods that work independently of the tissue type or preparation is complex, due to variations in nuclear morphology, staining intensity, cell density and nuclei aggregations. Machine learning-based segmentation methods can overcome these challenges, however high quality expert-annotated images are required for training. Currently, the limited number of annotated fluorescence image datasets publicly available do not cover a broad range of tissues and preparations. We present a comprehensive, annotated dataset including tightly aggregated nuclei of multiple tissues for the training of machine learning-based nuclear segmentation algorithms. The proposed dataset covers sample preparation methods frequently used in quantitative immunofluorescence microscopy. \n",
    "\n",
    "To spare some preprocessing time, this lab session starts with this pickle (download it and make it available for your notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"nuclei_cells_segmentations.pck\"\n",
    "f = open(fn, 'rb')\n",
    "X, Y = pickle.load(f)\n",
    "print(X.shape, Y.shape)\n",
    "N = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original image / Binary segmentation\")\n",
    "for i in (3,14):\n",
    "    figs, axs = plt.subplots(1,2)\n",
    "    axs[0].imshow(X[i].squeeze())\n",
    "    axs[1].imshow(Y[i].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b4881",
   "metadata": {},
   "source": [
    "This pickle contains a modified version of the dataset: \n",
    "- the same amount of images\n",
    "- all the images are resized to 128,128\n",
    "- the segmentation task is converted in a binary pixel classification: nuclei or not. \n",
    "\n",
    "The goal is now to train a U-Net on this dataset (70 images for training and 9 for \"test\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928ce5c",
   "metadata": {},
   "source": [
    "# U-Net overview\n",
    "\n",
    "Following the previous picture of U-Net, the network is composed of 3 parts: encoder, bottleneck, decoder. These three steps rely on a convolutional block (convolution, relu, convolution, relu) .\n",
    "\n",
    "The first step is the **encoder**. The goal is to compress the  \"geometrical\" information with local features (output channels). The encoder first applies a convolution of kernel size (3,3) to extract $F=64$ features. Then the spatial information is compressed using max-pooling (factor 2). The next step does the same:  extract $2\\times F=128$ features from the $F=64$, then compression with max-pooling. This operation is repeated 4 times in total to get at the end $F\\times 8 = 512$ channels that represent global features extracted from the input image. \n",
    "\n",
    "The **bottleneck** layer is a convolutional layer which doubles the number of channels. The idea is to create a \"dense\" representation of the image to gather both global and local features. \n",
    "\n",
    "The **decoder** part is similar to the encoder part but reversed. While we used max-pooling for downsampling in the encoder, the upsampling operation consists in **transposed convolution**. The goal is to increase (so upsample) the spatial dimensions of intermediate feature maps. \n",
    "\n",
    "The last peculiarity is the **output layer for classification** at the pixel level. In U-Net this last layer is (once again) a convolutional layer. This means that with the last hidden layer, we recover the same spatial dimension as the input with $F$ feature maps. The classification is carried out for each pixel independently, but the decision is based on $F$ features that encode global information. \n",
    "\n",
    "Before creating a U-Net model, we first study the new kind of layer `ConvTranspose2D`\n",
    "\n",
    "# ConvTranspose2D\n",
    "\n",
    "In pytorch, transposed convolution is achieved with the module ConvTranspose2D (for images or 2D objects).\n",
    "To better understand how it works, it can be useful to play with it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370d0e6",
   "metadata": {},
   "source": [
    "Since we will print many matrices to better understand this new operation, we first provide a helper function to better visualize the content of a torch tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmatrix(m, message=None):\n",
    "    \"\"\"Pretty print for matrices\n",
    "    Args: \n",
    "    - expect a torch Tensor\n",
    "    \n",
    "    Output: \n",
    "    The print \n",
    "    \n",
    "    Apply detach, squeeze, and numpy to the input tensor (not inplace) \n",
    "    \"\"\"\n",
    "    if message is not None: \n",
    "        print(message)\n",
    "    if len(m.shape) == 1: \n",
    "        print(m.squeeze().detach().numpy())\n",
    "    else: \n",
    "        print(tabulate(m.squeeze().detach().numpy(),tablefmt=\"fancy_grid\",floatfmt=\".3f\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64e260",
   "metadata": {},
   "source": [
    "Now we can use it to see what are the parameters of ConvTranspose2D: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ce71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, bias=True)\n",
    "ppmatrix(c.weight,\"weights: \")\n",
    "print(\"the bias: \")\n",
    "ppmatrix(c.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8c4ae",
   "metadata": {},
   "source": [
    "As you can see, the operation is parametrized by a convolution mask $\\mathbf{W}$ and one bias term. For one value $v$ in the input, we get as output $v\\times\\mathbf{W}+b$. As an illustration, we can consider a simple image with one channel. To start, it is easier to start without the bias term: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = th.zeros(1,1,2,2)\n",
    "c = nn.ConvTranspose2d(in_channels=1, out_channels=1, \n",
    "                       kernel_size=2, stride=1, \n",
    "                       bias=False)\n",
    "print(\"Weights: \",c.weight)\n",
    "print(\"Bias: \",c.bias)\n",
    "\n",
    "for i in range(4): \n",
    "    im = th.zeros(1,1,2,2)\n",
    "    im[0,0,i%2,i//2] = 1\n",
    "    print(\"-------------------\")\n",
    "    ppmatrix(im,\"image: \")\n",
    "    ppmatrix(c(im),\"output: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee049635",
   "metadata": {},
   "source": [
    "Try now with an image full of 1 and explain the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac04e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = th.ones(1,1,2,2)\n",
    "ppmatrix(c(im))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d0c0a",
   "metadata": {},
   "source": [
    "Now we can consider different stride: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.ConvTranspose2d(in_channels=1, out_channels=1, \n",
    "                       kernel_size=2, stride=2,  \n",
    "                       bias=False)\n",
    "print(c.weight)\n",
    "for i in range(4): \n",
    "    print(\"----------------\")\n",
    "    im = th.zeros(1,1,2,2)\n",
    "    im[0,0,i%2,i//2] = 1\n",
    "    ppmatrix(im,\"image:\")\n",
    "    print(c(im),\"output:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39e096",
   "metadata": {},
   "source": [
    "Try to understand the previous and the next examples and how it can be used to upsample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6213def",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = th.ones(1,1,2,2)\n",
    "ppmatrix(c(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb935c",
   "metadata": {},
   "source": [
    "More details can be found on this [blog post](https://towardsdatascience.com/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc904187",
   "metadata": {},
   "source": [
    "# Simple U-Net \n",
    "Now the goal is to implement a simplified version of U-Net (a short version of the \"U\"). \n",
    "The first step is the **encoder** that compresses (or reduces) the spatial dimension to create rich features that represent global information. \n",
    "\n",
    "## Step by step\n",
    "The encoder is composed of many successive blocks. One block is made of : \n",
    "- twice the sequence of a convolution (kernel size 3, stride 1, and F output channels), relu, batchnorm\n",
    "- followed by a max-pooling that reduces the dimensions by 2 (each spatial dimension is halved) \n",
    "The input image has one input channel and the convolutions generate $F$ output channel. $F$ will be a parameter and start with $F=4$. \n",
    "\n",
    "**TODO**: write the corresponding module and test if it works properly. Check the output dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f471e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c11c109",
   "metadata": {},
   "source": [
    "The **bottleneck** layer is a convolutional layer which doubles the number of channels. The idea is to create a \"dense\" representation of the image to gather both global and local features.\n",
    "\n",
    "**TODO**: write the corresponding module and test if it works properly. Check the output dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467c63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a822a4f",
   "metadata": {},
   "source": [
    "The **decoder** part is similar to the encoder part but reversed. While we used max-pooling for downsampling in the encoder, the upsampling operation consists in **transposed convolution**. The goal is to increase (so upsample) the spatial dimensions of intermediate feature maps while reducing the number of channels by a factor 2 for all of them. The important point is the residual connection. \n",
    "\n",
    "**TODO**: write the corresponding module and test if it works properly. Check the output dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e148d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0de5654",
   "metadata": {},
   "source": [
    "The last peculiarity is the output layer for classification at the pixel level. In U-Net this last layer is (once again) a convolutional layer. This means that with the last hidden layer, we recover the same spatial dimension as the input with $F$ feature maps. The classification is carried out for each pixel independently, but the decision is based on $F$ features that encode global information. \n",
    "\n",
    "**TODO**: write the corresponding module and test if it works properly. Check the output dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf338d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe4cedf",
   "metadata": {},
   "source": [
    "## A class for  simple U-Net\n",
    "\n",
    "Now we can merge all we did in the previous section to create a U-Net model (light version). \n",
    "\n",
    "**TODO**: \n",
    "- Write the `Module` that takes $F$ as hyper-parameter\n",
    "- Train it on the first 70 training images with $F=4$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6183bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0459786b",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "The evalution is important to really assess what we achieved and it can depend on the task and our purpose. \n",
    "\n",
    "Of course we can compute the accuracy (the % of well classified pixels), but it could be not enough. Here we can also plot the evaluation. \n",
    "\n",
    "**TODO**: \n",
    "- make a function that plots the evaluation result of an input image\n",
    "- try it on all the test images\n",
    "- maybe you can look at the accurate part, but what if you want to see which pixels belonging to a nucleous is missed ? \n",
    "- and for pixels wrongly affected to the nucleous class ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e22fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31dfb2ed",
   "metadata": {},
   "source": [
    "## Precision and recall\n",
    "\n",
    "For segmentation, it can be meaningful to look at the precision and recall. These two terms have very broad meaning depending on the purpose. Here we could say : \n",
    "- The precision for the class 1 (a nucleus) is the ratio between the number of true positives for the class 1  and the total of pixels classified as nuclei by the model. \n",
    "- The recall is the ratio between the number of pixels of class 1 correctly classified and the total of pixels classified that should be classified as nuclei. \n",
    "\n",
    "These measures depend on a threshold of the output score. While the \"natural\" threshold is $0$ on the output score (or $0.5$ if the model outputs probabilities), we can consider different tradeoff between precision and recall by varying the threshold. \n",
    "\n",
    "**TODO:**\n",
    "- Make a function which computes precision and recall for a given threshold\n",
    "- Plot the precision *vs* recall curve for a threshold varying between -5 and +5\n",
    "- Compare models with $F=4,16,32$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc021e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f64fe5f",
   "metadata": {},
   "source": [
    "# U-Net\n",
    "\n",
    "Now the goal is to implement U-Net. As a proposed roadmap we propose the following step: \n",
    "- a function to create a convolutional block\n",
    "- a module for the encoder\n",
    "- a module for the decoder\n",
    "- and a U-Net module to wrap everything\n",
    "\n",
    "The number of feature map ($F=64$ in the original work) must be a variable of the UNet. For the first round of experiment, we can use $F=8$.  \n",
    "\n",
    "\n",
    "**TODO:**\n",
    "- run the training on the 70 first images and spare the last 9 for evaluation\n",
    "- after the training process, look at the results on some training images and the evaluation ones. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065b84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47737a15",
   "metadata": {},
   "source": [
    "# Data Augmentation \n",
    "When the dataset is scarce, we can try data-augmentation. The idea is to apply transformation introduce diversity in the dataset with basic transformation. [Look at this page for more information](https://pytorch.org/vision/main/transforms.html). \n",
    "\n",
    "**TODO:**\n",
    "- Select a couple of transformation\n",
    "- Evaluate the impact of data-augmentation on our task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c815f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
